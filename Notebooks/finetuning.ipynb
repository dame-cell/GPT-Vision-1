{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning-notebook \n",
    "- Before you go through this code as mentioned before this project was inspired by Moondream so The fientuing code is similar\n",
    "- Except in our case we need to finetune both the Language Model and the MLP  OR multimodel-projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers timm einops datasets bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        self.data = load_dataset(\"google/docci\", trust_remote_code=True)[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return {\n",
    "            \"image\": sample[\"image\"], # Should be a PIL image\n",
    "            \"qa\": [\n",
    "                {\n",
    "                    \"question\": \"Describe this image.\",\n",
    "                    \"answer\": sample[\"description\"],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "datasets = {\n",
    "    \"train\": CaptchaDataset(\"train\"),\n",
    "    \"test\": CaptchaDataset(\"test\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"damerajee/GPTVision-1-ft\", trust_remote_code=True)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to repeat the training dataset. Increasing this may cause the model to overfit or\n",
    "# lose generalization due to catastrophic forgetting. Decreasing it may cause the model to underfit.\n",
    "EPOCHS = 1\n",
    "\n",
    "# Number of samples to process in each batch. Set this to the highest value that doesn't cause an\n",
    "# out-of-memory error. Decrease it if you're running out of memory.\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Number of batches to process before updating the model. You can use this to simulate a higher batch\n",
    "# size than your GPU can handle. Set this to 1 to disable gradient accumulation.\n",
    "GRAD_ACCUM_STEPS = 2\n",
    "\n",
    "# Learning rate for the Adam optimizer. Needs to be tuned on a case-by-case basis. As a general rule\n",
    "# of thumb, increase it by 1.4 times each time you double the effective batch size.\n",
    "#\n",
    "# Source: https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/\n",
    "#\n",
    "# Note that we linearly warm the learning rate up from 0.1 * LR to LR over the first 10% of the\n",
    "# training run, and then decay it back to 0.1 * LR over the last 90% of the training run using a\n",
    "# cosine schedule.\n",
    "LR = 1e-5\n",
    "DEVICE = \"cuda\"\n",
    "# Whether to use Weights and Biases for logging training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "import math\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from tqdm.notebooks import tqdm\n",
    "\n",
    "# Constants\n",
    "DEVICE = \"cuda\"\n",
    "ANSWER_EOS = \"<|endoftext|>\"\n",
    "IMG_TOKENS = 197\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [sample['image'].convert(\"RGB\") for sample in batch]\n",
    "\n",
    "    labels_acc = []\n",
    "    tokens_acc = []\n",
    "\n",
    "    for sample in batch:\n",
    "        toks = [tokenizer.bos_token_id]\n",
    "        labs = [-100] * (IMG_TOKENS + 1)\n",
    "\n",
    "        # Handle multiple QA pairs\n",
    "        for qa in sample['qa']:\n",
    "            q_t = tokenizer(\n",
    "                f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\",\n",
    "                add_special_tokens=False,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=104,\n",
    "            ).input_ids\n",
    "            toks.extend(q_t)\n",
    "            labs.extend([-100] * len(q_t))\n",
    "\n",
    "            a_t = tokenizer(\n",
    "                f\" {qa['answer']}{ANSWER_EOS}\",\n",
    "                add_special_tokens=False,\n",
    "                 padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=720,\n",
    "            ).input_ids\n",
    "            toks.extend(a_t)\n",
    "            labs.extend(a_t)\n",
    "\n",
    "        tokens_acc.append(toks)\n",
    "        labels_acc.append(labs)\n",
    "\n",
    "    max_len = max(len(labels) for labels in labels_acc)\n",
    "\n",
    "    attn_mask_acc = []\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        len_i = len(labels_acc[i])\n",
    "        pad_i = max_len - len_i\n",
    "\n",
    "        labels_acc[i].extend([-100] * pad_i)\n",
    "        tokens_acc[i].extend([tokenizer.eos_token_id] * pad_i)\n",
    "        attn_mask_acc.append([1] * len_i + [0] * pad_i)\n",
    "\n",
    "\n",
    "    return (\n",
    "        images,\n",
    "        torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n",
    "        torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n",
    "        torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n",
    "    )\n",
    "def evaluate_model(dataloader):\n",
    "    model.language_model.eval()\n",
    "    model.mlp.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images, tokens, labels, attn_mask = batch\n",
    "            tokens = tokens.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "            # Compute loss\n",
    "            img_embs = model.vision_encoder(images, device=DEVICE)\n",
    "            img_embs = model.mlp(img_embs)\n",
    "            tok_embs = model.language_model.get_input_embeddings()(tokens)\n",
    "            inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n",
    "\n",
    "            outputs = model.language_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                attention_mask=attn_mask,\n",
    "            )\n",
    "\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "def compute_loss(batch):\n",
    "    images, tokens, labels, attn_mask = batch\n",
    "    \n",
    "    tokens = tokens.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "    img_embs = model.vision_encoder(images, device=DEVICE)\n",
    "    \n",
    "    # Apply MLP to image embeddings\n",
    "    img_embs = model.mlp(img_embs)\n",
    "\n",
    "    tok_embs = model.language_model.get_input_embeddings()(tokens)\n",
    "\n",
    "    \n",
    "    # Concatenate image embeddings and token embeddings\n",
    "    inputs_embeds = torch.cat((tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    assert inputs_embeds.shape[1] == attn_mask.shape[1], \"Mismatch between embeddings and attention mask length.\"\n",
    "    \n",
    "  \n",
    "\n",
    "    outputs = model.language_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        attention_mask=attn_mask,\n",
    "    )\n",
    "\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lr_schedule(step, max_steps):\n",
    "    x = step / max_steps\n",
    "    if x < 0.1:\n",
    "        return 0.1 * LR + 0.9 * LR * x / 0.1\n",
    "    else:\n",
    "        return 0.1 * LR + 0.9 * LR * (1 + math.cos(math.pi * (x - 0.1))) / 2\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        datasets[\"train\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "    \"test\": DataLoader(\n",
    "        datasets[\"test\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "total_steps = EPOCHS * len(dataloaders[\"train\"]) // GRAD_ACCUM_STEPS\n",
    "model.language_model.train()\n",
    "model.mlp.train() \n",
    "model.language_model.transformer.gradient_checkpointing_enable()\n",
    "\n",
    "# Modify the optimizer to include MLP parameters\n",
    "optimizer = Adam8bit(\n",
    "    [\n",
    "        {\"params\": model.language_model.parameters()},\n",
    "        {\"params\": model.mlp.parameters()},  \n",
    "    ],\n",
    "    lr=LR * 0.1,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-6\n",
    ")\n",
    "\n",
    "print(\"WE ARE STRATING TO TRAIN\")\n",
    "i = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "        i += 1\n",
    "\n",
    "        loss = compute_loss(batch)\n",
    "        loss.backward()\n",
    "\n",
    "        if i % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lr = lr_schedule(i / GRAD_ACCUM_STEPS, total_steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            print(\"Evaluating model...\")\n",
    "            test_loss = evaluate_model(dataloaders[\"test\"])\n",
    "            print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            model.save_pretrained(f\"checkpoints/gptvision-ft_{i}\")\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "from IPython.display import display\n",
    "\n",
    "sample = datasets['train'][0]\n",
    "display(sample['image'])\n",
    "\n",
    "image = sample['image']\n",
    "image = image.resize((224,224))\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_p\": 0.6,\n",
    "    \"repetition_penalty\": 1.6,\n",
    "\n",
    "    }\n",
    "\n",
    "for qa in sample['qa']:\n",
    "    print('Question:', qa['question'])\n",
    "    print('Ground Truth:', qa['answer'])\n",
    "    print('GPT-Vision:', model.generate(\n",
    "        image = image ,\n",
    "        question =\"Describe this image.\",\n",
    "        max_new_tokens=256,\n",
    "        **gen_kwargs\n",
    "        \n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
